{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T14:14:31.068084Z",
     "start_time": "2024-09-25T14:14:31.004831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "labels_map = {\n",
    "    1: \"定子匝间短路\\正常电机\",\n",
    "    2: \"定子匝间短路\\短路0.1\",\n",
    "    3: \"定子匝间短路\\短路0.2\",\n",
    "    4: \"定子匝间短路\\短路0.5\",\n",
    "    5: \"定子匝间短路\\短路1\",\n",
    "    6: \"定子匝间短路\\短路2\",\n",
    "    7: \"定子匝间短路\\短路5\",\n",
    "    8: \"定子匝间短路\\短路10\",\n",
    "    9: \"定子匝间短路\\短路15\",\n",
    "    10: \"定子匝间短路\\短路20\",\n",
    "}\n",
    "\n",
    "file_path1=\"定子匝间短路\\正常电机电流.csv\"\n",
    "file_path1_=\"定子匝间短路\\正常电机转矩.csv\"\n",
    "file_path2=\"定子匝间短路\\短路0.1电流.csv\"\n",
    "file_path2_=\"定子匝间短路\\短路0.1转矩.csv\"\n",
    "file_path3=\"定子匝间短路\\短路0.2电流.csv\"\n",
    "file_path3_=\"定子匝间短路\\短路0.2转矩.csv\"\n",
    "file_path4=\"定子匝间短路\\短路0.5电流.csv\"\n",
    "file_path4_=\"定子匝间短路\\短路0.5转矩.csv\"\n",
    "file_path5=\"定子匝间短路\\短路1电流.csv\"\n",
    "file_path5_=\"定子匝间短路\\短路1转矩.csv\"\n",
    "file_path6=\"定子匝间短路\\短路2电流.csv\"\n",
    "file_path6_=\"定子匝间短路\\短路2转矩.csv\"\n",
    "file_path7=\"定子匝间短路\\短路5电流.csv\"\n",
    "file_path7_=\"定子匝间短路\\短路5转矩.csv\"\n",
    "file_path8=\"定子匝间短路\\短路10电流.csv\"\n",
    "file_path8_=\"定子匝间短路\\短路10转矩.csv\"\n",
    "file_path9=\"定子匝间短路\\短路15电流.csv\"\n",
    "file_path9_=\"定子匝间短路\\短路15转矩.csv\"\n",
    "file_path10=\"定子匝间短路\\短路20电流.csv\"\n",
    "file_path10_=\"定子匝间短路\\短路20转矩.csv\"\n",
    "\n",
    "file_paths = [\n",
    "    file_path1, file_path1_,\n",
    "    file_path2, file_path2_,\n",
    "    file_path3, file_path3_,\n",
    "    file_path4, file_path4_,\n",
    "    file_path5, file_path5_,\n",
    "    file_path6, file_path6_,\n",
    "    file_path7, file_path7_,\n",
    "    file_path8, file_path8_,\n",
    "    file_path9, file_path9_,\n",
    "    file_path10, file_path10_\n",
    "]\n",
    "\n",
    "# 创建一个空列表来存储 DataFrame\n",
    "dfs = []\n",
    "\n",
    "def read_df(df,df_):\n",
    "    df = df.iloc[:,1:].T\n",
    "    df_=df_.iloc[:,1:].T\n",
    "    df_append = pd.concat([df, df_], ignore_index=True, axis=0)\n",
    "    return  df_append\n",
    "\n",
    "# 循环遍历文件路径,读取 CSV 文件并创建 DataFrame\n",
    "for i in range(0, len(file_paths), 2):\n",
    "    df = pd.read_csv(file_paths[i])\n",
    "    df_ = pd.read_csv(file_paths[i+1])\n",
    "    data = read_df(df,df_)\n",
    "    dfs.append(data)\n",
    "\n",
    "\n",
    "# \n",
    "dfs = np.array(dfs)\n",
    "dfs = dfs.squeeze()\n",
    "dfs.shape    \n",
    "# dfs['label'] = labels_map.keys()\n",
    "x = torch.tensor(dfs,device='cuda',dtype=torch.float32)\n",
    "keys_list = list(labels_map.keys())\n",
    "labels = torch.tensor(np.array(keys_list), device='cuda', dtype=torch.float32)  # Use int64 for class labels\n",
    "x.shape, labels.shape\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # To match dimensions for residual connection\n",
    "        self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x if self.shortcut is None else self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(in_dim, in_dim)\n",
    "        self.key_layer = nn.Linear(in_dim, in_dim)\n",
    "        self.value_layer = nn.Linear(in_dim, in_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query_layer(x)  # Shape: [batch_size, seq_length, in_dim]\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "\n",
    "        attention_scores = torch.softmax(Q @ K.transpose(-2, -1), dim=-1)  # Shape: [batch_size, seq_length, seq_length]\n",
    "        output = attention_scores @ V  # Shape: [batch_size, seq_length, in_dim]\n",
    "        return output\n",
    "\n",
    "class ResSAModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResSAModel, self).__init__()\n",
    "        self.res_block1 = ResidualBlock(4, 256)  # Input channels set to 1\n",
    "        self.res_block2 = ResidualBlock(256, 64)  # Add another residual block\n",
    "        self.attention1 = SelfAttention(64)  # Attention over features after second block\n",
    "        self.fc = nn.Linear(64, num_classes)  # Output layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.res_block1(x)  # Shape: [batch_size, 16 (channels), seq_length]\n",
    "        x = self.res_block2(x)  # Shape: [batch_size, 32 (channels), seq_length]\n",
    "        \n",
    "        x = x.permute(0, 2, 1)   # Change shape to [batch_size, seq_length (1000), features (32)]\n",
    "        \n",
    "        x = self.attention1(x)   # Apply self-attention\n",
    "        \n",
    "        x = x.mean(dim=1)       # Global Average Pooling across sequence length\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x,labels,transform=None):\n",
    "        self.labels = labels  # Load labels from CSV\n",
    "        self.data_tensor = x\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.data_tensor.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the sample and its corresponding label (if applicable)\n",
    "        # Here we assume labels are just indices for demonstration purposes\n",
    "        sample = self.data_tensor[idx]\n",
    "        label = self.labels[idx]  # Replace with actual label if available\n",
    "        return sample, label\n",
    "    \n",
    "# Example usage:\n",
    "num_classes = 10  # Define number of output classes\n",
    "dataset = MyDataset(x,labels,transform=None)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "model = ResSAModel(num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "x.shape"
   ],
   "id": "6a3bf94cedd70be2",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 83\u001B[0m\n\u001B[0;32m     81\u001B[0m dfs\u001B[38;5;241m.\u001B[39mshape    \n\u001B[0;32m     82\u001B[0m \u001B[38;5;66;03m# dfs['label'] = labels_map.keys()\u001B[39;00m\n\u001B[1;32m---> 83\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdfs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     84\u001B[0m keys_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(labels_map\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[0;32m     85\u001B[0m labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(np\u001B[38;5;241m.\u001B[39marray(keys_list), device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)  \u001B[38;5;66;03m# Use int64 for class labels\u001B[39;00m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T14:14:02.683756Z",
     "start_time": "2024-09-25T14:14:01.423254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 1000  # Number of epochs to train\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "writer = SummaryWriter(\"runs/test_2\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for features, labels in data_loader:\n",
    "        features = features.float().to(device)\n",
    "        labels = labels.long().to(device)  # 确保标签是长整型\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features)  # Add channel dimension for Conv1d\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss / len(data_loader)    \n",
    "    writer.add_scalar('Loss/train', avg_loss, epoch) \n",
    "    \n",
    "    "
   ],
   "id": "277803258d4243dd",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 29\u001B[0m\n\u001B[0;32m     26\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     27\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m---> 29\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m running_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(data_loader)    \n\u001B[0;32m     31\u001B[0m writer\u001B[38;5;241m.\u001B[39madd_scalar(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoss/train\u001B[39m\u001B[38;5;124m'\u001B[39m, avg_loss, epoch)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def test_model(model, input_data):\n",
    "    # 将输入数据转换为 PyTorch 张量\n",
    "    input_tensor = torch.tensor(input_data, dtype=torch.float32).unsqueeze(0)  # 添加批次维度\n",
    "\n",
    "    # 将张量移动到相应的设备（CPU 或 GPU）\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # 不需要计算梯度\n",
    "        # 进行前向传播以获得输出\n",
    "        outputs = model(input_tensor.unsqueeze(1))  # 添加通道维度\n",
    "\n",
    "        # 获取预测结果\n",
    "        _, predicted = torch.max(outputs, 1)  # 获取最大值的索引作为预测标签\n",
    "\n",
    "    return predicted.item()  # 返回预测的标签\n",
    "# 假设您已经训练了模型并且它被称为 'model'\n",
    "\n",
    "# 输入一个长度为4004的列表\n",
    "input_data = dfs.iloc[4,:-1].values \n",
    "\n",
    "# 调用测试函数\n",
    "predicted_label = test_model(model, input_data)\n",
    "\n",
    "print(f'Predicted Label: {predicted_label}')"
   ],
   "id": "5e9a109313628930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4d98c74cf9afe96b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
